# This is an example configuration file that contains most useful parameter settings.
general:
  mode: verified-acc  # Compute verified accuracy.
  enable_complete_verification: False
model:
  name: docking
  input_ids: [0, 1, 2, 3, 4, 5]
  plt_ids:  [[4, 5]]
  path: "docking/docking_tanh64x64"
  plt_name: "abcrown_flowstar_docking_tanh64x64_v3"
  normalization: [1000.0, 1000.0, 0.5, 0.5, 1.0, 1.0]
flowstar:
  flowstar: "flowstar_1step_v1"
data:
  start: 0
  end: 120
  num_classes: 3
init:
  min: [24.9, 24.9, 0.13776233054248638, 0.13776233054248638, 0.1948253562373095, 0.272329386962147]
  max: [25.1, 25.1, 0.16531479665098364, 0.16531479665098364, 0.23379042748477138, 0.2729103458935699]
#specification:
  #norm: .inf  # Linf norm (can also be 2 or 1).
  #epsilon: 100.  # epsilon=2./255.
solver:
  alpha-crown:
    iteration: 100   # Number of iterations for alpha-CROWN optimization. Alpha-CROWN is used to compute all intermediate layer bounds before branch and bound starts.
    lr_alpha: 0.1    # Learning rate for alpha in alpha-CROWN. The default (0.1) is typically ok.
  beta-crown:
    batch_size: 2048  # Number of subdomains to compute in parallel in beta-CROWN. Increase if you run out of memory.
    lr_alpha: 0.01  # Learning rate for optimizing the alpha parameters, the default (0.01) is typically ok, but you can try to tune this parameter to get better lower bound. 
    lr_beta: 0.05  # Learning rate for optimizing the beta parameters, the default (0.05) is typically ok, but you can try to tune this parameter to get better lower bound.
    iteration: 20  # Number of iterations for beta-CROWN optimization. 20 is often sufficient, 50 or 100 can also be used.
bab:
  decision_thresh: 99999999
  timeout: 120  # Timeout threshold for branch and bound. Increase for verifying more points.
  branching:  # Parameters for branching heuristics.
    reduceop: min  # Reduction function for the branching heuristic scores, min or max. Using max can be better on some models.
    method: kfsb  # babsr is fast but less accurate; fsb is slow but most accurate; kfsb is usualy a balance.
    candidates: 3  # Number of candidates to consider in fsb and kfsb. More leads to slower but better branching. 3 is typically good enough.
