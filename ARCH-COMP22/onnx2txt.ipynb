{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 14:27:28.954498: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx2keras import onnx_to_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx2txt(model, txt_file):\n",
    "#     onnx_model = onnx.load(onnx_file)\n",
    "\n",
    "#     # load keras model from the h5 file\n",
    "#     model = onnx_to_keras(onnx_model, ['input'])\n",
    "    print(\"============model summary============\")\n",
    "    print(model.summary())\n",
    "\n",
    "    # get the input dim\n",
    "    input_dim = model.layers[0].get_config()['batch_input_shape'][-1]\n",
    "    # get the output dim\n",
    "    output_dim = model.layers[-1].get_config()['units']\n",
    "    # var to record number of hidden neurons\n",
    "    num_of_hidden_neurons = []\n",
    "    # var to record types of activation function\n",
    "    activations = []\n",
    "\n",
    "    for _layer in model.layers[1:]:\n",
    "        layer_config = _layer.get_config()\n",
    "        if 'layers' in layer_config:\n",
    "            # if there is a functional model in a layer, this layer actually\n",
    "            # contains layers that cannot be read from the model directly.\n",
    "            for _layer_in_model in _layer.layers[1:]:\n",
    "                layer_config = _layer_in_model.get_config()\n",
    "                # append the current layer's number of neurons\n",
    "                num_of_hidden_neurons.append(layer_config['units'])\n",
    "                # append the current layer's activation function type\n",
    "                activations.append(layer_config['activation'])\n",
    "        else:\n",
    "            if 'units' in layer_config:\n",
    "                # append the current layer's number of neurons\n",
    "                num_of_hidden_neurons.append(layer_config['units'])\n",
    "                # append the current layer's activation function type\n",
    "                activations.append(layer_config['activation'])\n",
    "            elif 'activation' in layer_config:\n",
    "                # for layers that only have the activation function, rewrite\n",
    "                # the latest activation function, probably linear, with current\n",
    "                # layer's activation function type.\n",
    "                activations[-1] = layer_config['activation']\n",
    "\n",
    "    # get the number of hidden layers\n",
    "    num_of_hidden_layer = len(activations) - 1\n",
    "\n",
    "    with open(txt_file, 'w') as output_file:\n",
    "        # write the neural network architecture\n",
    "        output_file.write('{}'.format(input_dim) + '\\n')\n",
    "        output_file.write('{}'.format(output_dim) + '\\n')\n",
    "        output_file.write('{}'.format(num_of_hidden_layer) + '\\n')\n",
    "\n",
    "        for _num_neurons in num_of_hidden_neurons[:-1]:\n",
    "            output_file.write('{}'.format(_num_neurons) + '\\n')\n",
    "\n",
    "        for _activation in activations:\n",
    "            if _activation == 'linear':\n",
    "                _activation = 'Affine'\n",
    "            output_file.write('{}'.format(_activation) + '\\n')\n",
    "\n",
    "        # write weights and biases\n",
    "        for _layer in model.layers[1:]:\n",
    "            layer_config = _layer.get_config()\n",
    "            if 'layers' in layer_config:\n",
    "                # if there is a functional model in a layer, this layer\n",
    "                # actually contains layers that cannot be read from the model\n",
    "                # directly.\n",
    "                for _layer_in_model in _layer.layers[1:]:\n",
    "                    weights, biases = _layer_in_model.get_weights()\n",
    "                    # wrtie weights\n",
    "                    for _col in range(weights.shape[1]):\n",
    "                        for _row in range(weights.shape[0]):\n",
    "                            output_file.write(\n",
    "                                '{}'.format(weights[_row, _col]) + '\\n'\n",
    "                            )\n",
    "                    # write biases\n",
    "                    for _idx_neuron in range(biases.shape[0]):\n",
    "                        output_file.write('{}'.format(\n",
    "                            biases[_idx_neuron]) + '\\n'\n",
    "                        )\n",
    "\n",
    "            else:\n",
    "                if 'units' in layer_config:\n",
    "                    weights, biases = _layer.get_weights()\n",
    "                    # wrtie weights\n",
    "                    for _col in range(weights.shape[1]):\n",
    "                        for _row in range(weights.shape[0]):\n",
    "                            output_file.write('{}'.format(\n",
    "                                weights[_row, _col]) + '\\n'\n",
    "                            )\n",
    "                    # write biases\n",
    "                    for _idx_neuron in range(biases.shape[0]):\n",
    "                        output_file.write('{}'.format(biases[_idx_neuron]) + '\\n')\n",
    "\n",
    "        # write default scalar and offset\n",
    "        output_file.write('{}'.format(0) + '\\n')\n",
    "        output_file.write('{}'.format(1) + '\\n')\n",
    "    print(\"============{} saved============\".format(txt_file))\n",
    "    print(\"============Done============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras2txt(model, txt_file):\n",
    "    # load keras model from the h5 file\n",
    "    # model = load_model(h5_file)\n",
    "    print(\"============keras model summary============\")\n",
    "    print(model.summary())\n",
    "\n",
    "    # get the input dim\n",
    "    if txt_file == './Airplane/controller_airplane_POLAR':\n",
    "        input_dim = 12\n",
    "    elif txt_file == './Sherlock-B10/controllerB_POLAR' or txt_file == './Sherlock-B9/controllerTora_POLAR':\n",
    "        input_dim = 4\n",
    "    else:\n",
    "        input_dim = model.layers[0].get_config()['batch_input_shape'][1]\n",
    "    # get the output dim\n",
    "    output_dim = model.layers[-1].get_config()['units']\n",
    "    # var to record number of hidden neurons\n",
    "    num_of_hidden_neurons = []\n",
    "    # var to record types of activation function\n",
    "    activations = []\n",
    "\n",
    "    for _layer in model.layers:\n",
    "        layer_config = _layer.get_config()\n",
    "        if 'layers' in layer_config:\n",
    "            # if there is a functional model in a layer, this layer actually\n",
    "            # contains layers that cannot be read from the model directly.\n",
    "            for _layer_in_model in _layer.layers[1:]:\n",
    "                layer_config = _layer_in_model.get_config()\n",
    "                # append the current layer's number of neurons\n",
    "                num_of_hidden_neurons.append(layer_config['units'])\n",
    "                # append the current layer's activation function type\n",
    "                activations.append(layer_config['activation'])\n",
    "        else:\n",
    "            if 'units' in layer_config:\n",
    "                # append the current layer's number of neurons\n",
    "                num_of_hidden_neurons.append(layer_config['units'])\n",
    "                # append the current layer's activation function type\n",
    "                activations.append(layer_config['activation'])\n",
    "            elif 'activation' in layer_config:\n",
    "                # for layers that only have the activation function, rewrite\n",
    "                # the latest activation function, probably linear, with current\n",
    "                # layer's activation function type.\n",
    "                activations[-1] = layer_config['activation']\n",
    "\n",
    "    # get the number of hidden layers\n",
    "    num_of_hidden_layer = len(activations) - 1\n",
    "\n",
    "    with open(txt_file, 'w') as output_file:\n",
    "        # write the neural network architecture\n",
    "        output_file.write('{}'.format(input_dim) + '\\n')\n",
    "        output_file.write('{}'.format(output_dim) + '\\n')\n",
    "        output_file.write('{}'.format(num_of_hidden_layer) + '\\n')\n",
    "\n",
    "        for _num_neurons in num_of_hidden_neurons[:-1]:\n",
    "            output_file.write('{}'.format(_num_neurons) + '\\n')\n",
    "\n",
    "        for _activation in activations:\n",
    "            if _activation == 'linear':\n",
    "                _activation = 'Affine'\n",
    "            output_file.write('{}'.format(_activation) + '\\n')\n",
    "\n",
    "        # write weights and biases\n",
    "        for _layer in model.layers:\n",
    "            layer_config = _layer.get_config()\n",
    "            if 'layers' in layer_config:\n",
    "                # if there is a functional model in a layer, this layer\n",
    "                # actually contains layers that cannot be read from the model\n",
    "                # directly.\n",
    "                for _layer_in_model in _layer.layers[1:]:\n",
    "                    weights, biases = _layer_in_model.get_weights()\n",
    "                    # wrtie weights\n",
    "                    for _col in range(weights.shape[1]):\n",
    "                        for _row in range(weights.shape[0]):\n",
    "                            output_file.write(\n",
    "                                '{}'.format(weights[_row, _col]) + '\\n'\n",
    "                            )\n",
    "                    # write biases\n",
    "                    for _idx_neuron in range(biases.shape[0]):\n",
    "                        output_file.write('{}'.format(\n",
    "                            biases[_idx_neuron]) + '\\n'\n",
    "                        )\n",
    "\n",
    "            else:\n",
    "                if 'units' in layer_config:\n",
    "                    weights, biases = _layer.get_weights()\n",
    "                    # wrtie weights\n",
    "                    for _col in range(weights.shape[1]):\n",
    "                        for _row in range(weights.shape[0]):\n",
    "                            output_file.write('{}'.format(\n",
    "                                weights[_row, _col]) + '\\n'\n",
    "                            )\n",
    "                    # write biases\n",
    "                    for _idx_neuron in range(biases.shape[0]):\n",
    "                        output_file.write('{}'.format(biases[_idx_neuron]) + '\\n')\n",
    "\n",
    "        # write default scalar and offset\n",
    "        output_file.write('{}'.format(0) + '\\n')\n",
    "        output_file.write('{}'.format(1) + '\\n')\n",
    "    print(\"============{} saved============\".format(txt_file))\n",
    "    print(\"============Done============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx2txt_withOnnxAndKerasModel(onnx_model, model, txt_file):\n",
    "#     onnx_model = onnx.load(onnx_file)\n",
    "\n",
    "#     # load keras model from the h5 file\n",
    "#     model = onnx_to_keras(onnx_model, ['input'])\n",
    "    print(\"============model summary============\")\n",
    "    print(model.summary())\n",
    "\n",
    "    # get the input dim\n",
    "    input_dim = model.layers[0].get_config()['batch_input_shape'][-1]\n",
    "    # get the output dim\n",
    "    output_dim = model.layers[-1].get_config()['units']\n",
    "    # var to record number of hidden neurons\n",
    "    num_of_hidden_neurons = []\n",
    "    # var to record types of activation function\n",
    "    activations = []\n",
    "\n",
    "    for _layer in model.layers[1:]:\n",
    "        layer_config = _layer.get_config()\n",
    "        if 'layers' in layer_config:\n",
    "            # if there is a functional model in a layer, this layer actually\n",
    "            # contains layers that cannot be read from the model directly.\n",
    "            for _layer_in_model in _layer.layers[1:]:\n",
    "                layer_config = _layer_in_model.get_config()\n",
    "                # append the current layer's number of neurons\n",
    "                num_of_hidden_neurons.append(layer_config['units'])\n",
    "                # append the current layer's activation function type\n",
    "                activations.append(layer_config['activation'])\n",
    "        else:\n",
    "            if 'units' in layer_config:\n",
    "                # append the current layer's number of neurons\n",
    "                num_of_hidden_neurons.append(layer_config['units'])\n",
    "                # append the current layer's activation function type\n",
    "                activations.append(layer_config['activation'])\n",
    "            elif 'activation' in layer_config:\n",
    "                # for layers that only have the activation function, rewrite\n",
    "                # the latest activation function, probably linear, with current\n",
    "                # layer's activation function type.\n",
    "                activations[-1] = layer_config['activation']\n",
    "\n",
    "    # get the number of hidden layers\n",
    "    num_of_hidden_layer = len(activations) - 1\n",
    "\n",
    "    with open(txt_file, 'w') as output_file:\n",
    "        # write the neural network architecture\n",
    "        output_file.write('{}'.format(input_dim) + '\\n')\n",
    "        output_file.write('{}'.format(output_dim) + '\\n')\n",
    "        output_file.write('{}'.format(num_of_hidden_layer) + '\\n')\n",
    "\n",
    "        for _num_neurons in num_of_hidden_neurons[:-1]:\n",
    "            output_file.write('{}'.format(_num_neurons) + '\\n')\n",
    "\n",
    "        for _activation in activations:\n",
    "            if _activation == 'linear':\n",
    "                _activation = 'Affine'\n",
    "            output_file.write('{}'.format(_activation) + '\\n')\n",
    "\n",
    "        # write weights and biases\n",
    "        for _layer in model.layers[1:]:\n",
    "            layer_config = _layer.get_config()\n",
    "            if 'layers' in layer_config:\n",
    "                # if there is a functional model in a layer, this layer\n",
    "                # actually contains layers that cannot be read from the model\n",
    "                # directly.\n",
    "                for _layer_in_model in _layer.layers[1:]:\n",
    "                    weights, biases = _layer_in_model.get_weights()\n",
    "                    # wrtie weights\n",
    "                    for _col in range(weights.shape[1]):\n",
    "                        for _row in range(weights.shape[0]):\n",
    "                            output_file.write(\n",
    "                                '{}'.format(weights[_row, _col]) + '\\n'\n",
    "                            )\n",
    "                    # write biases\n",
    "                    for _idx_neuron in range(biases.shape[0]):\n",
    "                        output_file.write('{}'.format(\n",
    "                            biases[_idx_neuron]) + '\\n'\n",
    "                        )\n",
    "\n",
    "            else:\n",
    "                if 'units' in layer_config:\n",
    "                    weights, biases = _layer.get_weights()\n",
    "                    # wrtie weights\n",
    "                    for _col in range(weights.shape[1]):\n",
    "                        for _row in range(weights.shape[0]):\n",
    "                            output_file.write('{}'.format(\n",
    "                                weights[_row, _col]) + '\\n'\n",
    "                            )\n",
    "                    # write biases\n",
    "                    for _idx_neuron in range(biases.shape[0]):\n",
    "                        output_file.write('{}'.format(biases[_idx_neuron]) + '\\n')\n",
    "\n",
    "        # write default scalar and offset\n",
    "        output_file.write('{}'.format(0) + '\\n')\n",
    "        output_file.write('{}'.format(1) + '\\n')\n",
    "    print(\"============{} saved============\".format(txt_file))\n",
    "    print(\"============Done============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load('./ACC/controller_5_20.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:onnx2keras:Converter is called.\n",
      "DEBUG:onnx2keras:List input shapes:\n",
      "DEBUG:onnx2keras:None\n",
      "DEBUG:onnx2keras:List inputs:\n",
      "DEBUG:onnx2keras:Input 0 -> input_AvgImg.\n",
      "DEBUG:onnx2keras:Input 1 -> Operation_1_W.\n",
      "DEBUG:onnx2keras:Input 2 -> Operation_1_B.\n",
      "DEBUG:onnx2keras:Input 3 -> Operation_2_W.\n",
      "DEBUG:onnx2keras:Input 4 -> Operation_2_B.\n",
      "DEBUG:onnx2keras:Input 5 -> Operation_3_W.\n",
      "DEBUG:onnx2keras:Input 6 -> Operation_3_B.\n",
      "DEBUG:onnx2keras:Input 7 -> Operation_4_W.\n",
      "DEBUG:onnx2keras:Input 8 -> Operation_4_B.\n",
      "DEBUG:onnx2keras:Input 9 -> Operation_5_W.\n",
      "DEBUG:onnx2keras:Input 10 -> Operation_5_B.\n",
      "DEBUG:onnx2keras:Input 11 -> linear_6_W.\n",
      "DEBUG:onnx2keras:Input 12 -> linear_6_B.\n",
      "DEBUG:onnx2keras:Input 13 -> input.\n",
      "DEBUG:onnx2keras:List outputs:\n",
      "DEBUG:onnx2keras:Output 0 -> linear_6.\n",
      "DEBUG:onnx2keras:Gathering weights to dictionary.\n",
      "DEBUG:onnx2keras:Found weight input_AvgImg with shape (1, 1, 1, 5).\n",
      "DEBUG:onnx2keras:Found weight Operation_1_W with shape (20, 5).\n",
      "DEBUG:onnx2keras:Found weight Operation_1_B with shape (20,).\n",
      "DEBUG:onnx2keras:Found weight Operation_2_W with shape (20, 20).\n",
      "DEBUG:onnx2keras:Found weight Operation_2_B with shape (20,).\n",
      "DEBUG:onnx2keras:Found weight Operation_3_W with shape (20, 20).\n",
      "DEBUG:onnx2keras:Found weight Operation_3_B with shape (20,).\n",
      "DEBUG:onnx2keras:Found weight Operation_4_W with shape (20, 20).\n",
      "DEBUG:onnx2keras:Found weight Operation_4_B with shape (20,).\n",
      "DEBUG:onnx2keras:Found weight Operation_5_W with shape (20, 20).\n",
      "DEBUG:onnx2keras:Found weight Operation_5_B with shape (20,).\n",
      "DEBUG:onnx2keras:Found weight linear_6_W with shape (1, 20).\n",
      "DEBUG:onnx2keras:Found weight linear_6_B with shape (1,).\n",
      "DEBUG:onnx2keras:Found input input with shape [1, 1, 5]\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Sub\n",
      "DEBUG:onnx2keras:node_name: input_Sub\n",
      "DEBUG:onnx2keras:node_params: {'broadcast': 1, 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name input).\n",
      "DEBUG:onnx2keras:Check input 1 (name input_AvgImg).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:sub:Convert inputs to Keras/TF layers if needed.\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 5), dtype=tf.float32, name=None), name='input_Sub/sub:0', description=\"created by layer 'input_Sub'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Gemm\n",
      "DEBUG:onnx2keras:node_name: Operation_1\n",
      "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'broadcast': 1, 'transA': 0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name input_Sub).\n",
      "DEBUG:onnx2keras:Check input 1 (name Operation_1_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name Operation_1_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
      "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
      "DEBUG:onnx2keras:gemm:Input units 5, output units 20.\n",
      "2022-08-23 12:15:35.445079: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-08-23 12:15:35.487247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 12:15:35.488335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 12:15:35.489432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:09:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 12:15:35.490159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 12:15:35.490183: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-08-23 12:15:35.494195: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-08-23 12:15:35.494237: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-08-23 12:15:35.495152: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-08-23 12:15:35.495388: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-08-23 12:15:35.495924: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-08-23 12:15:35.496757: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-08-23 12:15:35.496891: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-08-23 12:15:35.502391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2022-08-23 12:15:35.502777: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-23 12:15:36.008708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 12:15:36.009392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 12:15:36.010060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:09:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 12:15:36.010729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 12:15:36.015890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2022-08-23 12:15:36.015946: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-08-23 12:15:37.576820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-08-23 12:15:37.576854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 2 3 \n",
      "2022-08-23 12:15:37.576862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y Y Y \n",
      "2022-08-23 12:15:37.576866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N Y Y \n",
      "2022-08-23 12:15:37.576870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 2:   Y Y N Y \n",
      "2022-08-23 12:15:37.576874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 3:   Y Y Y N \n",
      "2022-08-23 12:15:37.588311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8722 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)\n",
      "2022-08-23 12:15:37.589483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7495 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2022-08-23 12:15:37.592540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 9432 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1)\n",
      "2022-08-23 12:15:37.593780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 9428 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1)\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 20), dtype=tf.float32, name=None), name='Operation_1/BiasAdd:0', description=\"created by layer 'Operation_1'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Relu\n",
      "DEBUG:onnx2keras:node_name: relu_1\n",
      "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name Operation_1).\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 20), dtype=tf.float32, name=None), name='relu_1/Relu:0', description=\"created by layer 'relu_1'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Gemm\n",
      "DEBUG:onnx2keras:node_name: Operation_2\n",
      "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'broadcast': 1, 'transA': 0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name relu_1).\n",
      "DEBUG:onnx2keras:Check input 1 (name Operation_2_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name Operation_2_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
      "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
      "DEBUG:onnx2keras:gemm:Input units 20, output units 20.\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 20), dtype=tf.float32, name=None), name='Operation_2/BiasAdd:0', description=\"created by layer 'Operation_2'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Relu\n",
      "DEBUG:onnx2keras:node_name: relu_2\n",
      "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name Operation_2).\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 20), dtype=tf.float32, name=None), name='relu_2/Relu:0', description=\"created by layer 'relu_2'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Gemm\n",
      "DEBUG:onnx2keras:node_name: Operation_3\n",
      "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'broadcast': 1, 'transA': 0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name relu_2).\n",
      "DEBUG:onnx2keras:Check input 1 (name Operation_3_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name Operation_3_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
      "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
      "DEBUG:onnx2keras:gemm:Input units 20, output units 20.\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 20), dtype=tf.float32, name=None), name='Operation_3/BiasAdd:0', description=\"created by layer 'Operation_3'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Relu\n",
      "DEBUG:onnx2keras:node_name: relu_3\n",
      "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name Operation_3).\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 20), dtype=tf.float32, name=None), name='relu_3/Relu:0', description=\"created by layer 'relu_3'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Gemm\n",
      "DEBUG:onnx2keras:node_name: Operation_4\n",
      "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'broadcast': 1, 'transA': 0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name relu_3).\n",
      "DEBUG:onnx2keras:Check input 1 (name Operation_4_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name Operation_4_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
      "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
      "DEBUG:onnx2keras:gemm:Input units 20, output units 20.\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 20), dtype=tf.float32, name=None), name='Operation_4/BiasAdd:0', description=\"created by layer 'Operation_4'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Relu\n",
      "DEBUG:onnx2keras:node_name: relu_4\n",
      "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name Operation_4).\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 20), dtype=tf.float32, name=None), name='relu_4/Relu:0', description=\"created by layer 'relu_4'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Gemm\n",
      "DEBUG:onnx2keras:node_name: Operation_5\n",
      "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'broadcast': 1, 'transA': 0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name relu_4).\n",
      "DEBUG:onnx2keras:Check input 1 (name Operation_5_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name Operation_5_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
      "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
      "DEBUG:onnx2keras:gemm:Input units 20, output units 20.\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 20), dtype=tf.float32, name=None), name='Operation_5/BiasAdd:0', description=\"created by layer 'Operation_5'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Relu\n",
      "DEBUG:onnx2keras:node_name: relu_5\n",
      "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name Operation_5).\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 20), dtype=tf.float32, name=None), name='relu_5/Relu:0', description=\"created by layer 'relu_5'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Gemm\n",
      "DEBUG:onnx2keras:node_name: linear_6\n",
      "DEBUG:onnx2keras:node_params: {'alpha': 1.0, 'beta': 1.0, 'broadcast': 1, 'transA': 0, 'transB': 1, 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name relu_5).\n",
      "DEBUG:onnx2keras:Check input 1 (name linear_6_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name linear_6_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:gemm:Convert GEMM with bias.\n",
      "DEBUG:onnx2keras:gemm:Transposing W matrix.\n",
      "DEBUG:onnx2keras:gemm:Input units 20, output units 1.\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 1), dtype=tf.float32, name=None), name='linear_6/BiasAdd:0', description=\"created by layer 'linear_6'\")\n"
     ]
    }
   ],
   "source": [
    "model = onnx_to_keras(onnx_model, ['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============model summary============\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 1, 1, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_Sub_const2 (Lambda)       (1, 1, 1, 5)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_Sub (Subtract)            (None, 1, 1, 5)      0           input[0][0]                      \n",
      "                                                                 input_Sub_const2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Operation_1 (Dense)             (None, 1, 1, 20)     120         input_Sub[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_1 (Activation)             (None, 1, 1, 20)     0           Operation_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Operation_2 (Dense)             (None, 1, 1, 20)     420         relu_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_2 (Activation)             (None, 1, 1, 20)     0           Operation_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Operation_3 (Dense)             (None, 1, 1, 20)     420         relu_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_3 (Activation)             (None, 1, 1, 20)     0           Operation_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Operation_4 (Dense)             (None, 1, 1, 20)     420         relu_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_4 (Activation)             (None, 1, 1, 20)     0           Operation_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Operation_5 (Dense)             (None, 1, 1, 20)     420         relu_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_5 (Activation)             (None, 1, 1, 20)     0           Operation_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "linear_6 (Dense)                (None, 1, 1, 1)      21          relu_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,821\n",
      "Trainable params: 1,821\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "============./ACC/controller_5_20_POLAR saved============\n",
      "============Done============\n"
     ]
    }
   ],
   "source": [
    "onnx2txt(model, './ACC/controller_5_20_POLAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./Airplane/controller_airplane.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               1300      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 13,546\n",
      "Trainable params: 13,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'dense_1',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'units': 100,\n",
       " 'activation': 'relu',\n",
       " 'use_bias': True,\n",
       " 'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "  'config': {'scale': 1.0,\n",
       "   'mode': 'fan_avg',\n",
       "   'distribution': 'uniform',\n",
       "   'seed': None}},\n",
       " 'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       " 'kernel_regularizer': None,\n",
       " 'bias_regularizer': None,\n",
       " 'activity_regularizer': None,\n",
       " 'kernel_constraint': None,\n",
       " 'bias_constraint': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============keras model summary============\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               1300      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 13,546\n",
      "Trainable params: 13,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "============./Airplane/controller_airplane_POLAR saved============\n",
      "============Done============\n"
     ]
    }
   ],
   "source": [
    "keras2txt(model, './Airplane/controller_airplane_POLAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sherlock-B10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               2500      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 3,502\n",
      "Trainable params: 3,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./Sherlock-B10/controllerB_nnv.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============keras model summary============\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               2500      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 3,502\n",
      "Trainable params: 3,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "============./Sherlock-B10/controllerB_POLAR saved============\n",
      "============Done============\n"
     ]
    }
   ],
   "source": [
    "keras2txt(model, './Sherlock-B10/controllerB_POLAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sherlock-B9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               500       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 20,801\n",
      "Trainable params: 20,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 14:27:37.323250: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-08-23 14:27:37.357313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 14:27:37.357986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 14:27:37.358692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:09:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 14:27:37.359393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 14:27:37.359417: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-08-23 14:27:37.363439: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-08-23 14:27:37.363480: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-08-23 14:27:37.364403: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-08-23 14:27:37.364635: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-08-23 14:27:37.365180: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-08-23 14:27:37.366028: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-08-23 14:27:37.366766: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-08-23 14:27:37.372255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2022-08-23 14:27:37.372629: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-23 14:27:37.793813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 14:27:37.794495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 14:27:37.795199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:09:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 14:27:37.795896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-08-23 14:27:37.801141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2022-08-23 14:27:37.801197: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-08-23 14:27:39.383027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-08-23 14:27:39.383059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 2 3 \n",
      "2022-08-23 14:27:39.383067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y Y Y \n",
      "2022-08-23 14:27:39.383071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N Y Y \n",
      "2022-08-23 14:27:39.383075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 2:   Y Y N Y \n",
      "2022-08-23 14:27:39.383079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 3:   Y Y Y N \n",
      "2022-08-23 14:27:39.389478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8722 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)\n",
      "2022-08-23 14:27:39.390355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7707 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2022-08-23 14:27:39.391237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 9450 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1)\n",
      "2022-08-23 14:27:39.392113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 9488 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./Sherlock-B9/controllerTora_nnv.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============keras model summary============\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               500       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 20,801\n",
      "Trainable params: 20,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "============./Sherlock-B9/controllerTora_POLAR saved============\n",
      "============Done============\n"
     ]
    }
   ],
   "source": [
    "keras2txt(model, './Sherlock-B9/controllerTora_POLAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:onnx2keras:Converter is called.\n",
      "DEBUG:onnx2keras:List input shapes:\n",
      "DEBUG:onnx2keras:None\n",
      "DEBUG:onnx2keras:List inputs:\n",
      "DEBUG:onnx2keras:Input 0 -> input.\n",
      "DEBUG:onnx2keras:List outputs:\n",
      "DEBUG:onnx2keras:Output 0 -> linear_5_Flatten.\n",
      "DEBUG:onnx2keras:Gathering weights to dictionary.\n",
      "DEBUG:onnx2keras:Found weight input_Mean with shape (1, 1, 4).\n",
      "DEBUG:onnx2keras:Found weight linear_1_W with shape (4, 1, 1, 4).\n",
      "DEBUG:onnx2keras:Found weight linear_1_B with shape (4,).\n",
      "DEBUG:onnx2keras:Found weight linear_2_W with shape (256, 4, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight linear_2_B with shape (256,).\n",
      "DEBUG:onnx2keras:Found weight Operation_3_W with shape (256, 256, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight Operation_3_B with shape (256,).\n",
      "DEBUG:onnx2keras:Found weight linear_4_W with shape (4, 256, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight linear_4_B with shape (4,).\n",
      "DEBUG:onnx2keras:Found weight linear_5_W with shape (4, 4, 1, 1).\n",
      "DEBUG:onnx2keras:Found weight linear_5_B with shape (4,).\n",
      "DEBUG:onnx2keras:Found input input with shape [1, 1, 4]\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Sub\n",
      "DEBUG:onnx2keras:node_name: input_Sub\n",
      "DEBUG:onnx2keras:node_params: {'broadcast': 1, 'axis': 1, 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name input).\n",
      "DEBUG:onnx2keras:Check input 1 (name input_Mean).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:sub:Convert inputs to Keras/TF layers if needed.\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1, 4), dtype=tf.float32, name=None), name='input_Sub/sub:0', description=\"created by layer 'input_Sub'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Conv\n",
      "DEBUG:onnx2keras:node_name: linear_1\n",
      "DEBUG:onnx2keras:node_params: {'group': 1, 'dilations': [1, 1], 'kernel_shape': [1, 4], 'pads': [0, 0, 0, 0], 'strides': [1, 1], 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name input_Sub).\n",
      "DEBUG:onnx2keras:Check input 1 (name linear_1_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name linear_1_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:conv:Conv with bias\n",
      "DEBUG:onnx2keras:conv:2D convolution\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 4, 1, 1), dtype=tf.float32, name=None), name='linear_1/BiasAdd:0', description=\"created by layer 'linear_1'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Conv\n",
      "DEBUG:onnx2keras:node_name: linear_2\n",
      "DEBUG:onnx2keras:node_params: {'group': 1, 'dilations': [1, 1], 'kernel_shape': [1, 1], 'pads': [0, 0, 0, 0], 'strides': [1, 1], 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name linear_1).\n",
      "DEBUG:onnx2keras:Check input 1 (name linear_2_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name linear_2_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:conv:Conv with bias\n",
      "DEBUG:onnx2keras:conv:2D convolution\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 256, 1, 1), dtype=tf.float32, name=None), name='linear_2/BiasAdd:0', description=\"created by layer 'linear_2'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Conv\n",
      "DEBUG:onnx2keras:node_name: Operation_3\n",
      "DEBUG:onnx2keras:node_params: {'group': 1, 'dilations': [1, 1], 'kernel_shape': [1, 1], 'pads': [0, 0, 0, 0], 'strides': [1, 1], 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name linear_2).\n",
      "DEBUG:onnx2keras:Check input 1 (name Operation_3_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name Operation_3_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:conv:Conv with bias\n",
      "DEBUG:onnx2keras:conv:2D convolution\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 256, 1, 1), dtype=tf.float32, name=None), name='Operation_3/BiasAdd:0', description=\"created by layer 'Operation_3'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Tanh\n",
      "DEBUG:onnx2keras:node_name: tanh_3\n",
      "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name Operation_3).\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 256, 1, 1), dtype=tf.float32, name=None), name='tanh_3/Tanh:0', description=\"created by layer 'tanh_3'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Conv\n",
      "DEBUG:onnx2keras:node_name: linear_4\n",
      "DEBUG:onnx2keras:node_params: {'group': 1, 'dilations': [1, 1], 'kernel_shape': [1, 1], 'pads': [0, 0, 0, 0], 'strides': [1, 1], 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name tanh_3).\n",
      "DEBUG:onnx2keras:Check input 1 (name linear_4_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name linear_4_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:conv:Conv with bias\n",
      "DEBUG:onnx2keras:conv:2D convolution\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 4, 1, 1), dtype=tf.float32, name=None), name='linear_4/BiasAdd:0', description=\"created by layer 'linear_4'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Conv\n",
      "DEBUG:onnx2keras:node_name: linear_5\n",
      "DEBUG:onnx2keras:node_params: {'group': 1, 'dilations': [1, 1], 'kernel_shape': [1, 1], 'pads': [0, 0, 0, 0], 'strides': [1, 1], 'change_ordering': False, 'name_policy': None}\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name linear_4).\n",
      "DEBUG:onnx2keras:Check input 1 (name linear_5_W).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:Check input 2 (name linear_5_B).\n",
      "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
      "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:conv:Conv with bias\n",
      "DEBUG:onnx2keras:conv:2D convolution\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 4, 1, 1), dtype=tf.float32, name=None), name='linear_5/BiasAdd:0', description=\"created by layer 'linear_5'\")\n",
      "DEBUG:onnx2keras:######\n",
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Converting ONNX operation\n",
      "DEBUG:onnx2keras:type: Flatten\n",
      "DEBUG:onnx2keras:node_name: linear_5_Flatten\n",
      "DEBUG:onnx2keras:node_params: {'axis': 1, 'change_ordering': False, 'name_policy': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:onnx2keras:...\n",
      "DEBUG:onnx2keras:Check if all inputs are available:\n",
      "DEBUG:onnx2keras:Check input 0 (name linear_5).\n",
      "DEBUG:onnx2keras:... found all, continue\n",
      "DEBUG:onnx2keras:flatten:Convert inputs to Keras/TF layers if needed.\n",
      "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name=None), name='linear_5_Flatten/Reshape:0', description=\"created by layer 'linear_5_Flatten'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 1, 1, 4)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_Sub_const2 (Lambda)       (1, 1, 4)            0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_Sub (Subtract)            (None, 1, 1, 4)      0           input[0][0]                      \n",
      "                                                                 input_Sub_const2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "linear_1 (Conv2D)               (None, 4, 1, 1)      20          input_Sub[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "linear_2 (Conv2D)               (None, 256, 1, 1)    1280        linear_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Operation_3 (Conv2D)            (None, 256, 1, 1)    65792       linear_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tanh_3 (Activation)             (None, 256, 1, 1)    0           Operation_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "linear_4 (Conv2D)               (None, 4, 1, 1)      1028        tanh_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "linear_5 (Conv2D)               (None, 4, 1, 1)      20          linear_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "linear_5_Flatten (Reshape)      (None, 4)            0           linear_5[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 68,140\n",
      "Trainable params: 68,140\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load('./Docking/bias_model.onnx')\n",
    "model = onnx_to_keras(onnx_model, ['input'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_modelweights = onnx_model.graph.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onnx_modelweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.01, 0.  , 0.  , 0.  ]]],\n",
       "\n",
       "\n",
       "       [[[0.  , 0.01, 0.  , 0.  ]]],\n",
       "\n",
       "\n",
       "       [[[0.  , 0.  , 2.  , 0.  ]]],\n",
       "\n",
       "\n",
       "       [[[0.  , 0.  , 0.  , 2.  ]]]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx.numpy_helper.to_array(onnx_modelweights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx2txt(model, './ACC/controller_5_20_POLAR')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
